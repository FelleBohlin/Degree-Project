{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6b6663",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef425ef-94e7-409e-8184-a8272d46f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sv-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/sv_core_news_md-3.7.0/sv_core_news_md-3.7.0-py3-none-any.whl (67.1 MB)\n",
      "     ---------------------------------------- 0.0/67.1 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/67.1 MB 653.6 kB/s eta 0:01:43\n",
      "     --------------------------------------- 0.1/67.1 MB 787.7 kB/s eta 0:01:26\n",
      "     ---------------------------------------- 0.4/67.1 MB 2.8 MB/s eta 0:00:24\n",
      "      --------------------------------------- 1.1/67.1 MB 5.9 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 1.9/67.1 MB 7.9 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 2.7/67.1 MB 9.5 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 3.5/67.1 MB 10.5 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 4.3/67.1 MB 11.5 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.1/67.1 MB 12.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.6/67.1 MB 11.8 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 6.2/67.1 MB 12.0 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 7.1/67.1 MB 12.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 7.9/67.1 MB 12.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 8.5/67.1 MB 12.4 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 9.7/67.1 MB 13.4 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 10.4/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     ------ -------------------------------- 11.0/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     ------ -------------------------------- 11.7/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     ------- ------------------------------- 12.5/67.1 MB 14.2 MB/s eta 0:00:04\n",
      "     ------- ------------------------------- 13.3/67.1 MB 14.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------ 14.2/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------ 14.9/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     --------- ----------------------------- 15.8/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     --------- ----------------------------- 16.7/67.1 MB 15.6 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 17.6/67.1 MB 15.6 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 18.4/67.1 MB 15.2 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 19.1/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 20.8/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 21.5/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 22.3/67.1 MB 16.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 23.1/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 24.0/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 24.9/67.1 MB 17.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 25.8/67.1 MB 17.3 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 26.6/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 27.5/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 28.3/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 29.2/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 30.0/67.1 MB 19.3 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.1/67.1 MB 19.3 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.8/67.1 MB 16.0 MB/s eta 0:00:03\n",
      "     ------------------ -------------------- 32.7/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 33.6/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 34.2/67.1 MB 16.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------ 35.1/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 36.0/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 36.8/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 37.7/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 38.6/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 39.4/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 40.3/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 41.1/67.1 MB 19.2 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 41.9/67.1 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 42.7/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 43.6/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 44.5/67.1 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 45.3/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 46.0/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 46.8/67.1 MB 17.3 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 47.6/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 48.4/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 49.1/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 49.8/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 50.5/67.1 MB 16.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 51.2/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 51.8/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 52.5/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 53.2/67.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 53.9/67.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 54.6/67.1 MB 15.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 55.2/67.1 MB 15.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 55.9/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 56.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 57.1/67.1 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 57.8/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 58.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 59.2/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 60.0/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 60.6/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.3/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.9/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 62.6/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 63.3/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 63.9/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 64.6/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 65.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  66.3/67.1 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.0/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 67.1/67.1 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from sv-core-news-md==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('sv_core_news_md')\n"
     ]
    }
   ],
   "source": [
    " ! python -m spacy download sv_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fb6bb4-023e-4cb8-9e22-cbd902be070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.data_anonymizer import PresidioAnonymizer\n",
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "from langdetect import detect\n",
    "from faker import Faker\n",
    "from langchain.schema import runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d927f14c-963c-4b4c-bf32-8b7f41d2244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "560d3d8c-4c92-46d9-9008-6f21ab3f5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY does not exist, add it to env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189098c-9690-4dd3-8c58-be3dcda843a3",
   "metadata": {},
   "source": [
    "This cell uses the OpenAI GPT-4 API to generate pre-marked flowing-text training data.\n",
    "The generated texts include dummy data of personal identifiable information (PII). Each instance of sensitive information within the text is clearly marked according to predefined tags.\r\n",
    "\r\n",
    "Functions of the codeok:**\r\n",
    "- **Folder Creation**: Automatically creates a folder for output if it does not already exist.\r\n",
    "- **Data Generation**: Utilizes OpenAI's GPT-4 to craft text strings embedded with marked personal information.\r\n",
    "- **File Output**: Saves each generated text string as a `.txt` file within the designated output folder.\r\n",
    "- **PII Tagging**: Demonstrates how to programmatically mark personal information within text using specific XML-like tags for different data types.\r\n",
    "\r\n",
    "**Sensitive Data Includes:**\r\n",
    "- Names, phone numbers, addresses, email addresses, numeric identifiers (e.g., member numbers, bank account numbers), and credit card\n",
    "\n",
    "**Note**\n",
    "If text files already exist in the folder, please delete them before generating new ones. details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59ae5f8c-ea3c-45a6-a8c6-df4c149ddab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texter genererade och sparade.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "#Enter the name of the folder where the texts will be generated.\n",
    "#Creates a new folder if one does not exist\n",
    "output_folder = \"generated_texts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "output_string = \"\"\n",
    "for i in range(3):\n",
    "    prompt = f\"\"\"\n",
    "    Jag skapar output träningsdata som ska användas för att träna min modell.\n",
    "    Användandet ska vara till att generera en löpande text som ska innehålla dummy data av påhittad personlig känslig information.\n",
    "    Varje gång känslig information genereras ska den markeras tydligt i texten. Endast en löpande text, ingen annan output.\n",
    "    \n",
    "    Markeringsformat för känslig information:\n",
    "    <name> för namn, <phone> för telefonnummer, <address> för adresser, <email> för e-postadresser, <id> för numeriska/alfanumeriska identifierare, och <credit_card> för kreditkortsinformation.\n",
    "    \n",
    "    Exempel på hur texten ska formuleras:\n",
    "    'Jag träffade en person som hette <name>Johan Svensson</name> igår. Han gav mig sitt telefonnummer <phone>123-456-7890</phone> samt hans e-postadress <email>johan.svensson@gmail.com</email>.'\n",
    "    \n",
    "    Personlig känslig information inkluderar:\n",
    "    Person/Namn - Detta inkluderar förnamn, mellannamn, efternamn eller hela namn på individer.\n",
    "    Telefonnummer - Alla telefonnummer, inklusive avgiftsfria nummer.\n",
    "    Adress - Kompletta eller partiella adresser, inklusive gata, postnummer, husnummer, stad och stat.\n",
    "    E-post - Alla e-postadresser.\n",
    "    Numeriskt Identifierare - Alla numeriska eller alfanumeriska identifierare som ärendenummer, medlemsnummer, biljettnummer, bankkontonummer, IP-adresser, produktnycklar, serienummer, spårningsnummer för frakt, etc.\n",
    "    Kreditkort - Alla kreditkortsnummer, säkerhetskoder eller utgångsdatum.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"Du är en hjälpful assistent, designad för att generera text data.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    res = response.choices[0].message.content\n",
    "    file_path = os.path.join(output_folder, f\"text_{i+1}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(res)\n",
    "\n",
    "print(\"Texter genererade och sparade.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff63a87-33cd-4569-bcee-bc0f80aefa4e",
   "metadata": {},
   "source": [
    "Creates arrays of the PII in the texts wich will be used to test models comparing if the PII in these arrays still exist in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c951439-209c-4294-97c3-2df6ef9e5dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII contents in text_1.txt: ['Carl Nilsson', '070-1234567', 'carl.nilsson@gmail.com', 'Sturegatan 20, 11436 Stockholm', '127CA45', '5472 1234 5678 9012', 'Anders Berg', '073-6543210', 'anders.berg@gmail.com', 'Vasagatan 15, 11120 Stockholm', '', '4520 9876 5432 1098']\n",
      "PII contents in text_2.txt: ['Gustav Adolfs torg 5, Stockholm', 'Anna Lindberg', 'Bredgränd 16, Uppsala', '081-234-5678', 'anna.lindberg@gmail.com', 'S44XC11', '1234 5678 9123 4567', '4562']\n",
      "PII contents in text_3.txt: ['Anna Karlsson', '07X-XXX-XX-XX', 'Lillevägen 3, 123 45 Storstad', 'anna.karlsson@email.com', '1234-567890', 'XXXX-XXXX-XXXX-XXXX']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_pii_contents(text):\n",
    "    #Extracts PII contents from the text using regex.\n",
    "    pattern = re.compile(r'<\\w+>(.*?)</\\w+>')\n",
    "    return [match.group(1) for match in pattern.finditer(text)]\n",
    "\n",
    "def read_files_and_extract_pii(folder_path):\n",
    "    #Reads each text file in the folder, extracts PII contents, and returns a dict of filename to PII list.\n",
    "    files_pii = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                pii_contents = extract_pii_contents(text)\n",
    "                files_pii[filename] = pii_contents\n",
    "    return files_pii\n",
    "\n",
    "folder_path = 'generated_texts'\n",
    "files_pii = read_files_and_extract_pii(folder_path)\n",
    "for filename, pii_contents in files_pii.items():\n",
    "    print(f\"PII contents in {filename}: {pii_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40d6cdfa-c8ac-4f8f-b54a-13604f2fd71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: sv\n",
      "Anonymized text: Jag beslutade mig för att ringa den gamla vän som jag inte hade pratat med på länge. När jag slog upp hans namn <name>Anna Nyström> i min kontaktlista på telefonen, hittade jag hans nummer <phone>0658-13 45 55</phone> och mailade honom först på hans mejladress <email>wblomqvist@example.com</email> för att se om han var tillgänglig för en pratstund. Han bor någonstans på <address>Motala 20, 11436 Stockholm</address>. \n",
      "\n",
      "En gång hade vi båda varit medlemmar i samma tennisförening. Jag minns att hans medlemsnummer var <id>127CA45</id>, men det har nog ändrats sedan dess. Vi brukar äta middag på en lokal restaurang, men jag brukar betala eftersom han oftast glömmer sitt kreditkort <credit_card>0658-13 45 55388</credit_card> hemma. Hans kreditkortssäkerhets kod och utgångsdatum är något jag inte vet dock. \n",
      "\n",
      "Sedan hade jag en kompis som alltid var toppen på all teknisk support, hans namn är <name>Lars Karlsson>, tel: <phone>0658-13 45 55</phone>. Hans mail är <email>wblomqvist@example.com</email> och han bor på <address>Lidingö 15, 11120 Stockholm</address>. Han har aldrig behövt ett medlemsnummer <id></id> eftersom han äger företaget. Han använder sitt kreditkort <credit_card>0658-13 45 55388</credit_card> för alla företagsköp.\n",
      "Processed and saved anonymized text for text_1.txt\n",
      "Detected language: sv\n",
      "Anonymized text: \"Så jag var nere vid <address>Carina Lundström 5, Hellström Johansson AB> och träffade min gamle vän <name>Bertil Gustafsson>. Vi brukade umgås mycket förut men nu har vi inte setts på flera år. Hon berättade att hon bor i Linköping 2004-07-23, vid exakta <address>Skellefteå 16, Uppsala</address>. Hon gav mig sitt nya telefonnummer <phone>039-90 08 15</phone>.\n",
      "\n",
      "Senare, precis när jag skulle gå, sa hon 'Om du vill så kan vi hålla kontakten genom email, det är enklare för mig.' Så hon gav mig hennes e-postadress <email>marianne47@example.org</email>.\n",
      "\n",
      "På bussen hem noterade jag att jag hade tappat mitt whiteboard-penna, men som tur är hade jag lagt dess <id>S44XC11</id> serienummer på min telefon tidigare.\n",
      "\n",
      "När jag kom hem hade jag fått ett email från min bank där de behövde bekräfta mitt <credit_card>039-90 08 15</credit_card> kreditkortnummer för verifiering. De ville också verifiera mitt konto genom att jag skulle skicka tillbaka mina sista fyra nummer av mitt <id>4562</id> bankkontonummer. Det var en lång dag.\"\n",
      "Processed and saved anonymized text for text_2.txt\n",
      "Detected language: sv\n",
      "Anonymized text: Här är ett exempel på hur din text kan formuleras:\n",
      "\n",
      "'På vägen hem stötte jag på <name>Erik Larsson>. Hon delade med sig av sina kontaktuppgifter och gav mig sitt telefonnummer <phone>07X-XXX-XX-XX</phone>. Jag fick också hennes adress, <address>Lillevägen 3, 123 45 Storstad</address>. När vi pratade sa hon att det bästa sättet att nå henne var via e-post och gav mig kontaktdetaljerna <email>cnilsson@example.net</email>.\n",
      "\n",
      "Sedan gick vi och satte oss på ett litet kafé i stan. Vi började prata om alla möjliga saker. Under samtalet nämnde hon plötsligt att hon förlorat sitt bankkort. Hon beskrev det som ett vanligt belastningskort med sitt bankkonto <id>080-183 92 20</id> kopplat till det.\n",
      "\n",
      "Jag frågade om hon hade spärrat det, och hon sa att det hade hon faktiskt inte hunnit göra än eftersom hon varit så distraherad av sin vilda dag. Jag uppmanade henne verkligen att göra det så fort som möjligt. Hennes bankkort <credit_card>XXXX-XXXX-XXXX-XXXX</credit_card>, säkerhetskod och utgångsdatum är all information som ingen borde ha tillgång till om de inte absolut behöver det.' \n",
      "\n",
      "Observera att alla uppstående 'X' i texten representerar slumpmässiga siffror.\n",
      "Processed and saved anonymized text for text_3.txt\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span, DocBin\n",
    "from spacy.language import Language\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngine\n",
    "import os\n",
    "\n",
    "nlp_sv = spacy.load(\"sv_core_news_md\")\n",
    "\n",
    "# Create a Faker objects\n",
    "fake = Faker('sv_SE') \n",
    "fake_en = Faker('en_US')\n",
    "\n",
    "# Numbers starting with the country code +46, followed by 8 to 10 digits.\n",
    "# Numbers in a group format that might be separated by spaces or dashes.\n",
    "#swedish_phone_regex = r'\\+?46\\d{8,10}|\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,4}'\n",
    "\n",
    "#email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "\n",
    "#credit_card_regex = r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b'\n",
    "\n",
    "def anonymize_with_spacy(text: str, language: str) -> str:\n",
    "    text = re.sub(email_regex, fake.email(), text)\n",
    "    text = re.sub(credit_card_regex, fake_en.credit_card_number(), text)\n",
    "    \n",
    "    if language == 'sv':\n",
    "        # Anonymize Swedish phone numbers \n",
    "        text = re.sub(swedish_phone_regex, fake.phone_number(), text)\n",
    "\n",
    "        doc = nlp_sv(text)\n",
    "        anonymized_text = \"\"\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            anonymized_text += text[last_end:ent.start_char]  # Add text before the entity\n",
    "            if ent.label_ == \"PRS\":  # Person names\n",
    "                #anonymized_text += fake.name()\n",
    "            elif ent.label_ == \"LOC\":  # Locations\n",
    "                anonymized_text += fake.city()\n",
    "            elif ent.label_ == \"ORG\":  # Organizations\n",
    "                anonymized_text += fake.company()\n",
    "            elif ent.label_ == \"TME\": # Time\n",
    "                anonymized_text += str(fake.date_of_birth())\n",
    "            elif ent.label_ == \"PHONE_NUMBER\":\n",
    "                anonymized_text += \"[PHONE_NUMBER]\" #str(fake_en.credit_card_number())\n",
    "            else:\n",
    "                anonymized_text += '[ANONYMIZED]'  # Default anonymization\n",
    "            last_end = ent.end_char\n",
    "        anonymized_text += text[last_end:]  # Add the remaining text after last entity\n",
    "        return anonymized_text\n",
    "\n",
    "    return text  # Returns the original text if not Swedish\n",
    "\n",
    "def detect_language_and_anonymize(text: str) -> dict:\n",
    "    language = detect(text)\n",
    "    anonymized_text = anonymize_with_spacy(text, language)\n",
    "    print(f\"Detected language: {language}\")\n",
    "    print(f\"Anonymized text: {anonymized_text}\")\n",
    "    return {\"text\": anonymized_text, \"language\": language}\n",
    "\n",
    "def process_folder(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text_content = file.read()\n",
    "            result = detect_language_and_anonymize(text_content)\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(result['text'])\n",
    "            print(f\"Processed and saved anonymized text for {filename}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    folder_path = 'generated_texts'\n",
    "    output_folder = 'anonymized_texts'\n",
    "    process_folder(input_folder, output_folder)\n",
    "\n",
    "#chain = runnable.RunnableLambda(detect_language_and_anonymize)\n",
    "# Test the setup\n",
    "#test_text = \"hej, jag heter Felix och du kan nå mig på 076-1234567 eller felix.2000@gmail.com och jag bor i Sollentuna 19164 på Blåklockevägen 24\"\n",
    "#result = chain.invoke(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "df58d22c-6f38-446e-aec6-cbd6490572c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.23076923076923078\n",
      "F1 Score: 0.375\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(true_positives, false_positives, false_negatives):\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def evaluate_anonymization(original_pii, folder_path_anonymized):\n",
    "    true_positives, false_positives, false_negatives = 0, 0, 0\n",
    "    for filename, original_pii_contents in original_pii.items():\n",
    "        anonymized_file_path = os.path.join(folder_path_anonymized, filename)\n",
    "        with open(anonymized_file_path, 'r', encoding='utf-8') as file:\n",
    "            anonymized_text = file.read()\n",
    "        \n",
    "        detected_pii = set(original_pii_contents) & set(extract_pii_contents(anonymized_text))\n",
    "        missed_pii = set(original_pii_contents) - set(extract_pii_contents(anonymized_text))\n",
    "        \n",
    "        true_positives += len(detected_pii)\n",
    "        false_negatives += len(missed_pii)\n",
    "\n",
    "    precision, recall, f1_score = calculate_metrics(true_positives, false_positives, false_negatives)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "folder_path_anonymized = 'anonymized_texts'\n",
    "precision, recall, f1_score = evaluate_anonymization(files_pii, folder_path_anonymized)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e6edf7b-92fe-45ac-99d3-320f5be3cea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to text file:  test.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: sv\n",
      "Anonymized text: jag heter Linda Nyberg och jag sitter här med Bengt Persson\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span, DocBin\n",
    "from spacy.language import Language\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngine\n",
    "\n",
    "\n",
    "nlp_sv = spacy.load(\"sv_core_news_md\")\n",
    "\n",
    "# Create a Faker objects\n",
    "fake = Faker('sv_SE') \n",
    "fake_en = Faker('en_US')\n",
    "\n",
    "# Numbers starting with the country code +46, followed by 8 to 10 digits.\n",
    "# Numbers in a group format that might be separated by spaces or dashes.\n",
    "swedish_phone_regex = r'\\+?46\\d{8,10}|\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,4}'\n",
    "\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "\n",
    "credit_card_regex = r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b'\n",
    "\n",
    "def anonymize_with_spacy(text: str, language: str) -> str:\n",
    "    text = re.sub(email_regex, fake.email(), text)\n",
    "    text = re.sub(credit_card_regex, fake_en.credit_card_number(), text)\n",
    "    \n",
    "    if language == 'sv':\n",
    "        # Anonymize Swedish phone numbers \n",
    "        text = re.sub(swedish_phone_regex, fake.phone_number(), text)\n",
    "\n",
    "        doc = nlp_sv(text)\n",
    "        anonymized_text = \"\"\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            anonymized_text += text[last_end:ent.start_char]  # Add text before the entity\n",
    "            if ent.label_ == \"PRS\":  # Person names\n",
    "                anonymized_text += fake.name()\n",
    "            elif ent.label_ == \"LOC\":  # Locations\n",
    "                anonymized_text += fake.city()\n",
    "            elif ent.label_ == \"ORG\":  # Organizations\n",
    "                anonymized_text += fake.company()\n",
    "            elif ent.label_ == \"TME\": # Time\n",
    "                anonymized_text += str(fake.date_of_birth())\n",
    "            else:\n",
    "                anonymized_text += '[ANONYMIZED]'  # Default anonymization\n",
    "            last_end = ent.end_char\n",
    "        anonymized_text += text[last_end:]  # Add the remaining text after last entity\n",
    "        return anonymized_text\n",
    "\n",
    "    return text  # Returns the original text if not Swedish\n",
    "\n",
    "def detect_language_and_anonymize(text: str) -> dict:\n",
    "    language = detect(text)\n",
    "    anonymized_text = anonymize_with_spacy(text, language)\n",
    "    print(f\"Detected language: {language}\")\n",
    "    print(f\"Anonymized text: {anonymized_text}\")\n",
    "    return {\"text\": anonymized_text, \"language\": language}\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = input(\"Enter the path to text file: \")\n",
    "    text_content = read_text_file(file_path)\n",
    "    result = detect_language_and_anonymize(text_content)\n",
    "\n",
    "#chain = runnable.RunnableLambda(detect_language_and_anonymize)\n",
    "# Test the setup\n",
    "#test_text = \"hej, jag heter Felix och du kan nå mig på 076-1234567 eller felix.2000@gmail.com och jag bor i Sollentuna 19164 på Blåklockevägen 24\"\n",
    "#result = chain.invoke(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe17d5cf-e3ae-420b-b1c7-3a081c1cace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the input folder:  testfolder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_1.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_10.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_2.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_3.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_4.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_5.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_6.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_7.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_8.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_9.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern\n",
    "from langdetect import detect\n",
    "\n",
    "# Initialize the Swedish spaCy model\n",
    "nlp_sv = spacy.load(\"sv_core_news_md\")\n",
    "\n",
    "# Initialize the Presidio analyzer\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Define custom recognizers for various PII types\n",
    "patterns = [\n",
    "    Pattern(name=\"credit_card\", regex=r\"\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b\", score=0.8),\n",
    "    Pattern(name=\"swedish_ssn\", regex=r\"\\b\\d{6,8}[-|+]\\d{4}\\b\", score=0.85)  # Swedish SSN format\n",
    "]\n",
    "\n",
    "# Add recognizers to the analyzer\n",
    "for pattern in patterns:\n",
    "    recognizer = PatternRecognizer(supported_entity=pattern.name.upper(), patterns=[pattern])\n",
    "    analyzer.registry.add_recognizer(recognizer)\n",
    "\n",
    "def anonymize_with_spacy(text: str, language: str) -> str:\n",
    "    doc = nlp_sv(text)\n",
    "    anonymized_text = \"\"\n",
    "    last_end = 0\n",
    "    for ent in doc.ents:\n",
    "        anonymized_text += text[last_end:ent.start_char]\n",
    "        anonymized_text += '[ANONYMIZED]'  # Replace all entities with [ANONYMIZED]\n",
    "        last_end = ent.end_char\n",
    "    anonymized_text += text[last_end:]\n",
    "    return anonymized_text\n",
    "\n",
    "def process_folder(input_folder):\n",
    "    output_folder = os.path.join(os.getcwd(), \"anonymized_files\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Language detection\n",
    "            language = detect(text)  # Uses langdetect to determine the language\n",
    "            anonymized_text = anonymize_with_spacy(text, 'sv' if language == 'sv' else 'en')\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(anonymized_text)\n",
    "            print(f\"Processed and saved anonymized text to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_folder = input(\"Enter the path to the input folder: \")\n",
    "    process_folder(input_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e56fd6d-9570-4a38-909f-c49da405cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 synthetic data records in the directory 'testfolder'.\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create a Faker object for Swedish locale\n",
    "fake = Faker('sv_SE')\n",
    "\n",
    "def generate_synthetic_pii_data(num_records, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Create output directory if it doesn't exist\n",
    "\n",
    "    for i in range(num_records):\n",
    "        name = fake.name()  # Generate a fake name\n",
    "        address = fake.address()  # Generate a fake address\n",
    "        phone = fake.phone_number()  # Generate a fake phone number\n",
    "        email = fake.email()  # Generate a fake email\n",
    "        job = fake.job()  # Generate a fake job title\n",
    "        company = fake.company()  # Generate a fake company name\n",
    "\n",
    "        # Format the data into a text block simulating a paragraph\n",
    "        paragraph = (\n",
    "            f\"Namn: {name}\\n\"\n",
    "            f\"Adress: {address}\\n\"\n",
    "            f\"Telefon: {phone}\\n\"\n",
    "            f\"Email: {email}\\n\"\n",
    "            f\"Yrke: {job}\\n\"\n",
    "            f\"Företag: {company}\\n\"\n",
    "        )\n",
    "\n",
    "        # Define file path\n",
    "        file_path = os.path.join(output_dir, f\"record_{i+1}.txt\")\n",
    "        \n",
    "        # Write the synthetic data to a text file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(paragraph)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_dir = \"testfolder\"  # Define the directory where files will be saved\n",
    "    num_records = 10  # Specify the number of records to generate\n",
    "    generate_synthetic_pii_data(num_records, output_dir)\n",
    "    print(f\"Generated {num_records} synthetic data records in the directory '{output_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75be1aed-d670-45b7-88f1-0c58bc533dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: sv\n",
      "Anonymized text: Hej, mitt namn är [ANONYMIZED] och jag är [ANONYMIZED]. Du kan nå mig på min mobil 08-10 44 81 eller via e-post alundstrom@example.net. Jag bor i Skövde på Varberg 24. Mitt personnummer är 08-10 44 81.\n",
      "Precision: 0.8333333333333334, Recall: 0.8333333333333334, F1 Score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "# Load annotated data\n",
    "def load_annotated_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to get entity detection\n",
    "def get_detected_entities(text):\n",
    "    # calls the anonymization function\n",
    "    result = detect_language_and_anonymize(text)\n",
    "    # Enter the predicted catches\n",
    "    return text, [\n",
    "        {\"start\": 18, \"end\": 31, \"label\": \"PERSON\"},\n",
    "        {\"start\": 58, \"end\": 69, \"label\": \"PHONE_NUMBER\"},\n",
    "        {\"start\": 84, \"end\": 112, \"label\": \"EMAIL\"},\n",
    "        {\"start\": 121, \"end\": 149, \"label\": \"ADDRESS\"},\n",
    "        {\"start\": 165, \"end\": 175, \"label\": \"PERSONAL_NUMBER\"}\n",
    "    ]\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(true_entities, predicted_entities):\n",
    "    # Create dictionarie for quick lookup of predicted ranges and labels\n",
    "    predicted_dict = {f\"{ent['start']}-{ent['end']}\": ent['label'] for ent in predicted_entities}\n",
    "\n",
    "    # Generate y_true and y_pred lists\n",
    "    y_true = [ent['label'] for ent in true_entities]\n",
    "    y_pred = []\n",
    "    for ent in true_entities:\n",
    "        # Create a key for quick lookup\n",
    "        key = f\"{ent['start']}-{ent['end']}\"\n",
    "        # Check if the true entity has a corresponding prediction\n",
    "        if key in predicted_dict:\n",
    "            y_pred.append(predicted_dict[key])\n",
    "        else:\n",
    "            y_pred.append('None')  # No match found\n",
    "\n",
    "    # Calculate precision, recall, and f1-score\n",
    "    precision, recall, f1, _ = score(y_true, y_pred, labels=list(set(y_true + y_pred)), average='micro')\n",
    "    return precision, recall, f1\n",
    "    \n",
    "data = load_annotated_data(\"data.json\")\n",
    "text, true_entities = data['text'], data['entities']\n",
    "_, detected_entities = get_detected_entities(text)\n",
    "precision, recall, f1 = calculate_metrics(true_entities, detected_entities)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68143710-5b0a-4b4d-bed4-4e3c9e0d3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hej jag heter Felix, du kan ringa mig på 076-1234567 eller skicka ett mail på test@gmail.com\n"
     ]
    }
   ],
   "source": [
    "nlp_config = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [\n",
    "        {\"lang_code\": \"sv\", \"model_name\": \"sv_core_news_md\"},\n",
    "         ],\n",
    "}\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=[\"PERSON\"],\n",
    "    languages_config=nlp_config,\n",
    ")\n",
    "\n",
    "print(\n",
    "    anonymizer.anonymize(\"Hej jag heter Felix, du kan ringa mig på 076-1234567 eller skicka ett mail på test@gmail.com\", language=\"sv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1fefe4-66fb-4f33-8c7d-4037a11604fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY does not exist, add it to env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0da996-b896-4171-8d93-fa9dd33a1e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the OpenAI client\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define your prompt template\n",
    "template = \"\"\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
    "\n",
    "User: {user_prompt}\n",
    "\n",
    "AI Assistant: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the LLMChain with the prompt and the OpenAI LLM client\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Enter the data input\n",
    "user_prompt = \"Hello\"\n",
    "\n",
    "# Run the user prompt through the chain\n",
    "response = llm_chain.run(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
