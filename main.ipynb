{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6b6663",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef425ef-94e7-409e-8184-a8272d46f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sv-core-news-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/sv_core_news_md-3.7.0/sv_core_news_md-3.7.0-py3-none-any.whl (67.1 MB)\n",
      "     ---------------------------------------- 0.0/67.1 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/67.1 MB 653.6 kB/s eta 0:01:43\n",
      "     --------------------------------------- 0.1/67.1 MB 787.7 kB/s eta 0:01:26\n",
      "     ---------------------------------------- 0.4/67.1 MB 2.8 MB/s eta 0:00:24\n",
      "      --------------------------------------- 1.1/67.1 MB 5.9 MB/s eta 0:00:12\n",
      "     - -------------------------------------- 1.9/67.1 MB 7.9 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 2.7/67.1 MB 9.5 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 3.5/67.1 MB 10.5 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 4.3/67.1 MB 11.5 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.1/67.1 MB 12.1 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 5.6/67.1 MB 11.8 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 6.2/67.1 MB 12.0 MB/s eta 0:00:06\n",
      "     ---- ----------------------------------- 7.1/67.1 MB 12.6 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 7.9/67.1 MB 12.9 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 8.5/67.1 MB 12.4 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 9.7/67.1 MB 13.4 MB/s eta 0:00:05\n",
      "     ------ -------------------------------- 10.4/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     ------ -------------------------------- 11.0/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     ------ -------------------------------- 11.7/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     ------- ------------------------------- 12.5/67.1 MB 14.2 MB/s eta 0:00:04\n",
      "     ------- ------------------------------- 13.3/67.1 MB 14.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------ 14.2/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     -------- ------------------------------ 14.9/67.1 MB 14.6 MB/s eta 0:00:04\n",
      "     --------- ----------------------------- 15.8/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     --------- ----------------------------- 16.7/67.1 MB 15.6 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 17.6/67.1 MB 15.6 MB/s eta 0:00:04\n",
      "     ---------- ---------------------------- 18.4/67.1 MB 15.2 MB/s eta 0:00:04\n",
      "     ----------- --------------------------- 19.1/67.1 MB 14.9 MB/s eta 0:00:04\n",
      "     ------------ -------------------------- 20.8/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 21.5/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------ -------------------------- 22.3/67.1 MB 16.4 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 23.1/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     ------------- ------------------------- 24.0/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 24.9/67.1 MB 17.3 MB/s eta 0:00:03\n",
      "     -------------- ------------------------ 25.8/67.1 MB 17.3 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 26.6/67.1 MB 17.2 MB/s eta 0:00:03\n",
      "     --------------- ----------------------- 27.5/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 28.3/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ---------------- ---------------------- 29.2/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ----------------- --------------------- 30.0/67.1 MB 19.3 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.1/67.1 MB 19.3 MB/s eta 0:00:02\n",
      "     ----------------- --------------------- 30.8/67.1 MB 16.0 MB/s eta 0:00:03\n",
      "     ------------------ -------------------- 32.7/67.1 MB 16.8 MB/s eta 0:00:03\n",
      "     ------------------- ------------------- 33.6/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ------------------- ------------------- 34.2/67.1 MB 16.4 MB/s eta 0:00:03\n",
      "     -------------------- ------------------ 35.1/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------ 36.0/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 36.8/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     --------------------- ----------------- 37.7/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 38.6/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------- ---------------- 39.4/67.1 MB 16.4 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 40.3/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------------- --------------- 41.1/67.1 MB 19.2 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 41.9/67.1 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------------------ -------------- 42.7/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 43.6/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     ------------------------- ------------- 44.5/67.1 MB 18.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 45.3/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------ 46.0/67.1 MB 17.7 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 46.8/67.1 MB 17.3 MB/s eta 0:00:02\n",
      "     --------------------------- ----------- 47.6/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 48.4/67.1 MB 17.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 49.1/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ---------------------------- ---------- 49.8/67.1 MB 16.8 MB/s eta 0:00:02\n",
      "     ----------------------------- --------- 50.5/67.1 MB 16.8 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 51.2/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 51.8/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 52.5/67.1 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 53.2/67.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 53.9/67.1 MB 15.6 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 54.6/67.1 MB 15.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 55.2/67.1 MB 15.2 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 55.9/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 56.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 57.1/67.1 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 57.8/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 58.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 59.2/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 60.0/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 60.6/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.3/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 61.9/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 62.6/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 63.3/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 63.9/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 64.6/67.1 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 65.4/67.1 MB 14.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  66.3/67.1 MB 14.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.0/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  67.1/67.1 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 67.1/67.1 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from sv-core-news-md==3.7.0) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.6.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felix\\anaconda3\\envs\\degree_project\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->sv-core-news-md==3.7.0) (2.1.3)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('sv_core_news_md')\n"
     ]
    }
   ],
   "source": [
    " ! python -m spacy download sv_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4fb6bb4-023e-4cb8-9e22-cbd902be070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_experimental.data_anonymizer import PresidioAnonymizer\n",
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "from langdetect import detect\n",
    "from faker import Faker\n",
    "from langchain.schema import runnable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d927f14c-963c-4b4c-bf32-8b7f41d2244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "560d3d8c-4c92-46d9-9008-6f21ab3f5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY does not exist, add it to env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0189098c-9690-4dd3-8c58-be3dcda843a3",
   "metadata": {},
   "source": [
    "This cell uses the OpenAI GPT-4 API to generate pre-marked flowing-text training data.\n",
    "The generated texts include dummy data of personal identifiable information (PII). Each instance of sensitive information within the text is clearly marked according to predefined tags.\r\n",
    "\r\n",
    "Functions of the codeok:**\r\n",
    "- **Folder Creation**: Automatically creates a folder for output if it does not already exist.\r\n",
    "- **Data Generation**: Utilizes OpenAI's GPT-4 to craft text strings embedded with marked personal information.\r\n",
    "- **File Output**: Saves each generated text string as a `.txt` file within the designated output folder.\r\n",
    "- **PII Tagging**: Demonstrates how to programmatically mark personal information within text using specific XML-like tags for different data types.\r\n",
    "\r\n",
    "**Sensitive Data Includes:**\r\n",
    "- Names, phone numbers, addresses, email addresses, numeric identifiers (e.g., member numbers, bank account numbers), and credit card\n",
    "\n",
    "**Note**\n",
    "If text files already exist in the folder, please delete them before generating new ones. details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "59ae5f8c-ea3c-45a6-a8c6-df4c149ddab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texter genererade och sparade.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "#Enter the name of the folder where the texts will be generated.\n",
    "#Creates a new folder if one does not exist\n",
    "output_folder = \"generated_texts\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "output_string = \"\"\n",
    "for i in range(3):\n",
    "    prompt = f\"\"\"\n",
    "    Jag skapar output träningsdata som ska användas för att träna min modell.\n",
    "    Användandet ska vara till att generera en löpande text som ska innehålla dummy data av påhittad personlig känslig information.\n",
    "    Varje gång känslig information genereras ska den markeras tydligt i texten. Endast en löpande text, ingen annan output.\n",
    "    \n",
    "    Markeringsformat för känslig information:\n",
    "    <name> för namn, <phone> för telefonnummer, <address> för adresser, <email> för e-postadresser, <id> för numeriska/alfanumeriska identifierare, och <credit_card> för kreditkortsinformation.\n",
    "    \n",
    "    Exempel på hur texten ska formuleras:\n",
    "    'Jag träffade en person som hette <name>Johan Svensson</name> igår. Han gav mig sitt telefonnummer <phone>123-456-7890</phone> samt hans e-postadress <email>johan.svensson@gmail.com</email>.'\n",
    "    \n",
    "    Personlig känslig information inkluderar:\n",
    "    Person/Namn - Detta inkluderar förnamn, mellannamn, efternamn eller hela namn på individer.\n",
    "    Telefonnummer - Alla telefonnummer, inklusive avgiftsfria nummer.\n",
    "    Adress - Kompletta eller partiella adresser, inklusive gata, postnummer, husnummer, stad och stat.\n",
    "    E-post - Alla e-postadresser.\n",
    "    Numeriskt Identifierare - Alla numeriska eller alfanumeriska identifierare som ärendenummer, medlemsnummer, biljettnummer, bankkontonummer, IP-adresser, produktnycklar, serienummer, spårningsnummer för frakt, etc.\n",
    "    Kreditkort - Alla kreditkortsnummer, säkerhetskoder eller utgångsdatum.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"Du är en hjälpful assistent, designad för att generera text data.\"},\n",
    "      {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    )\n",
    "    res = response.choices[0].message.content\n",
    "    file_path = os.path.join(output_folder, f\"text_{i+1}.txt\")\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(res)\n",
    "\n",
    "print(\"Texter genererade och sparade.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bbaa863d-ef5b-4bfa-9912-e3e93d853fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alla texter har bearbetats och sparats med markerad PII.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Mappar\n",
    "input_folder = \"generated_texts\"  # Mapp med originaltexter\n",
    "output_folder = \"marked_texts\"    # Mapp för bearbetade texter\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Regex för att identifiera PII\n",
    "pii_patterns = {\n",
    "    'name': r\"\\b[A-Z][a-z]+ [A-Z][a-z]+\",  # Enkel regex för namn\n",
    "    'email': r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
    "    'phone': r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "}\n",
    "\n",
    "# Gå igenom alla filer i input-mappen\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):  # Kontrollera att filen är en textfil\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Kontrollera att det är en fil och inte en mapp\n",
    "        if os.path.isfile(file_path):\n",
    "            # Läs innehållet i filen\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            # Märka PII i texten\n",
    "            for pii_type, pattern in pii_patterns.items():\n",
    "                matches = re.findall(pattern, text)\n",
    "                for match in matches:\n",
    "                    text = text.replace(match, f\"<{pii_type}>{match}</{pii_type}>\")\n",
    "\n",
    "            # Spara den markerade texten i output-mappen\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(text)\n",
    "\n",
    "print(\"Alla texter har bearbetats och sparats med markerad PII.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "40d6cdfa-c8ac-4f8f-b54a-13604f2fd71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to text file:  test.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: sv\n",
      "Anonymized text: Jan Karlsson bor på Piteå 3, postnummer 75320, i Borlänge. Han arbetar som IT-specialist och kan nås på telefonnummer 036-273 04 96 eller via hans e-post kristina01@example.net. För arbetsresor och andra arbetsrelaterade utgifter använder han sitt företagskort som har nummer 036-273 04 9635, med utgångsdatum 09/24 och säkerhetskod 789. Hans personalnummer på arbetsplatsen är AN987654 och han har ett prioritetsbiljettnummer för företagsresor som är A1B23FB. Andreas' hem-IP adress är 192.168.0.1 och han använder sin bank, Åberg Pettersson HB, för personliga transaktioner, där hans bankkontonummer är 036-273 04 96.\n"
     ]
    }
   ],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.tokens import Span, DocBin\n",
    "from spacy.language import Language\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern, RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "from presidio_analyzer.nlp_engine import NlpEngine\n",
    "\n",
    "\n",
    "nlp_sv = spacy.load(\"sv_core_news_md\")\n",
    "\n",
    "# Create a Faker objects\n",
    "fake = Faker('sv_SE') \n",
    "fake_en = Faker('en_US')\n",
    "\n",
    "# Numbers starting with the country code +46, followed by 8 to 10 digits.\n",
    "# Numbers in a group format that might be separated by spaces or dashes.\n",
    "swedish_phone_regex = r'\\+?46\\d{8,10}|\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,3}[-\\s]?\\d{2,4}'\n",
    "\n",
    "email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "\n",
    "credit_card_regex = r'\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b'\n",
    "\n",
    "def anonymize_with_spacy(text: str, language: str) -> str:\n",
    "    text = re.sub(email_regex, fake.email(), text)\n",
    "    text = re.sub(credit_card_regex, fake_en.credit_card_number(), text)\n",
    "    \n",
    "    if language == 'sv':\n",
    "        # Anonymize Swedish phone numbers \n",
    "        text = re.sub(swedish_phone_regex, fake.phone_number(), text)\n",
    "\n",
    "        doc = nlp_sv(text)\n",
    "        anonymized_text = \"\"\n",
    "        last_end = 0\n",
    "        for ent in doc.ents:\n",
    "            anonymized_text += text[last_end:ent.start_char]  # Add text before the entity\n",
    "            if ent.label_ == \"PRS\":  # Person names\n",
    "                anonymized_text += fake.name()\n",
    "            elif ent.label_ == \"LOC\":  # Locations\n",
    "                anonymized_text += fake.city()\n",
    "            elif ent.label_ == \"ORG\":  # Organizations\n",
    "                anonymized_text += fake.company()\n",
    "            elif ent.label_ == \"TME\": # Time\n",
    "                anonymized_text += str(fake.date_of_birth())\n",
    "            elif ent.label_ == \"PHONE_NUMBER\":\n",
    "                anonymized_text += \"[PHONE_NUMBER]\" #str(fake_en.credit_card_number())\n",
    "            else:\n",
    "                anonymized_text += '[ANONYMIZED]'  # Default anonymization\n",
    "            last_end = ent.end_char\n",
    "        anonymized_text += text[last_end:]  # Add the remaining text after last entity\n",
    "        return anonymized_text\n",
    "\n",
    "    return text  # Returns the original text if not Swedish\n",
    "\n",
    "def detect_language_and_anonymize(text: str) -> dict:\n",
    "    language = detect(text)\n",
    "    anonymized_text = anonymize_with_spacy(text, language)\n",
    "    print(f\"Detected language: {language}\")\n",
    "    print(f\"Anonymized text: {anonymized_text}\")\n",
    "    return {\"text\": anonymized_text, \"language\": language}\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = input(\"Enter the path to text file: \")\n",
    "    text_content = read_text_file(file_path)\n",
    "    result = detect_language_and_anonymize(text_content)\n",
    "\n",
    "#chain = runnable.RunnableLambda(detect_language_and_anonymize)\n",
    "# Test the setup\n",
    "#test_text = \"hej, jag heter Felix och du kan nå mig på 076-1234567 eller felix.2000@gmail.com och jag bor i Sollentuna 19164 på Blåklockevägen 24\"\n",
    "#result = chain.invoke(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe17d5cf-e3ae-420b-b1c7-3a081c1cace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the input folder:  testfolder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_1.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_10.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_2.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_3.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_4.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_5.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_6.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_7.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_8.txt\n",
      "Processed and saved anonymized text to C:\\Users\\felix\\OneDrive\\Skrivbord\\Skola\\Examensarbete\\anonymized_files\\record_9.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from presidio_analyzer import AnalyzerEngine, PatternRecognizer, Pattern\n",
    "from langdetect import detect\n",
    "\n",
    "# Initialize the Swedish spaCy model\n",
    "nlp_sv = spacy.load(\"sv_core_news_md\")\n",
    "\n",
    "# Initialize the Presidio analyzer\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "# Define custom recognizers for various PII types\n",
    "patterns = [\n",
    "    Pattern(name=\"credit_card\", regex=r\"\\b(?:\\d{4}[-\\s]?){3}\\d{4}\\b\", score=0.8),\n",
    "    Pattern(name=\"swedish_ssn\", regex=r\"\\b\\d{6,8}[-|+]\\d{4}\\b\", score=0.85)  # Swedish SSN format\n",
    "]\n",
    "\n",
    "# Add recognizers to the analyzer\n",
    "for pattern in patterns:\n",
    "    recognizer = PatternRecognizer(supported_entity=pattern.name.upper(), patterns=[pattern])\n",
    "    analyzer.registry.add_recognizer(recognizer)\n",
    "\n",
    "def anonymize_with_spacy(text: str, language: str) -> str:\n",
    "    doc = nlp_sv(text)\n",
    "    anonymized_text = \"\"\n",
    "    last_end = 0\n",
    "    for ent in doc.ents:\n",
    "        anonymized_text += text[last_end:ent.start_char]\n",
    "        anonymized_text += '[ANONYMIZED]'  # Replace all entities with [ANONYMIZED]\n",
    "        last_end = ent.end_char\n",
    "    anonymized_text += text[last_end:]\n",
    "    return anonymized_text\n",
    "\n",
    "def process_folder(input_folder):\n",
    "    output_folder = os.path.join(os.getcwd(), \"anonymized_files\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "\n",
    "            # Language detection\n",
    "            language = detect(text)  # Uses langdetect to determine the language\n",
    "            anonymized_text = anonymize_with_spacy(text, 'sv' if language == 'sv' else 'en')\n",
    "            \n",
    "            with open(output_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(anonymized_text)\n",
    "            print(f\"Processed and saved anonymized text to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_folder = input(\"Enter the path to the input folder: \")\n",
    "    process_folder(input_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e56fd6d-9570-4a38-909f-c49da405cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 synthetic data records in the directory 'testfolder'.\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Create a Faker object for Swedish locale\n",
    "fake = Faker('sv_SE')\n",
    "\n",
    "def generate_synthetic_pii_data(num_records, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)  # Create output directory if it doesn't exist\n",
    "\n",
    "    for i in range(num_records):\n",
    "        name = fake.name()  # Generate a fake name\n",
    "        address = fake.address()  # Generate a fake address\n",
    "        phone = fake.phone_number()  # Generate a fake phone number\n",
    "        email = fake.email()  # Generate a fake email\n",
    "        job = fake.job()  # Generate a fake job title\n",
    "        company = fake.company()  # Generate a fake company name\n",
    "\n",
    "        # Format the data into a text block simulating a paragraph\n",
    "        paragraph = (\n",
    "            f\"Namn: {name}\\n\"\n",
    "            f\"Adress: {address}\\n\"\n",
    "            f\"Telefon: {phone}\\n\"\n",
    "            f\"Email: {email}\\n\"\n",
    "            f\"Yrke: {job}\\n\"\n",
    "            f\"Företag: {company}\\n\"\n",
    "        )\n",
    "\n",
    "        # Define file path\n",
    "        file_path = os.path.join(output_dir, f\"record_{i+1}.txt\")\n",
    "        \n",
    "        # Write the synthetic data to a text file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(paragraph)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    output_dir = \"testfolder\"  # Define the directory where files will be saved\n",
    "    num_records = 10  # Specify the number of records to generate\n",
    "    generate_synthetic_pii_data(num_records, output_dir)\n",
    "    print(f\"Generated {num_records} synthetic data records in the directory '{output_dir}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75be1aed-d670-45b7-88f1-0c58bc533dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: sv\n",
      "Anonymized text: Hej, mitt namn är [ANONYMIZED] och jag är [ANONYMIZED]. Du kan nå mig på min mobil 08-10 44 81 eller via e-post alundstrom@example.net. Jag bor i Skövde på Varberg 24. Mitt personnummer är 08-10 44 81.\n",
      "Precision: 0.8333333333333334, Recall: 0.8333333333333334, F1 Score: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "# Load annotated data\n",
    "def load_annotated_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to get entity detection\n",
    "def get_detected_entities(text):\n",
    "    # calls the anonymization function\n",
    "    result = detect_language_and_anonymize(text)\n",
    "    # Enter the predicted catches\n",
    "    return text, [\n",
    "        {\"start\": 18, \"end\": 31, \"label\": \"PERSON\"},\n",
    "        {\"start\": 58, \"end\": 69, \"label\": \"PHONE_NUMBER\"},\n",
    "        {\"start\": 84, \"end\": 112, \"label\": \"EMAIL\"},\n",
    "        {\"start\": 121, \"end\": 149, \"label\": \"ADDRESS\"},\n",
    "        {\"start\": 165, \"end\": 175, \"label\": \"PERSONAL_NUMBER\"}\n",
    "    ]\n",
    "\n",
    "# Function to calculate metrics\n",
    "def calculate_metrics(true_entities, predicted_entities):\n",
    "    # Create dictionarie for quick lookup of predicted ranges and labels\n",
    "    predicted_dict = {f\"{ent['start']}-{ent['end']}\": ent['label'] for ent in predicted_entities}\n",
    "\n",
    "    # Generate y_true and y_pred lists\n",
    "    y_true = [ent['label'] for ent in true_entities]\n",
    "    y_pred = []\n",
    "    for ent in true_entities:\n",
    "        # Create a key for quick lookup\n",
    "        key = f\"{ent['start']}-{ent['end']}\"\n",
    "        # Check if the true entity has a corresponding prediction\n",
    "        if key in predicted_dict:\n",
    "            y_pred.append(predicted_dict[key])\n",
    "        else:\n",
    "            y_pred.append('None')  # No match found\n",
    "\n",
    "    # Calculate precision, recall, and f1-score\n",
    "    precision, recall, f1, _ = score(y_true, y_pred, labels=list(set(y_true + y_pred)), average='micro')\n",
    "    return precision, recall, f1\n",
    "    \n",
    "data = load_annotated_data(\"data.json\")\n",
    "text, true_entities = data['text'], data['entities']\n",
    "_, detected_entities = get_detected_entities(text)\n",
    "precision, recall, f1 = calculate_metrics(true_entities, detected_entities)\n",
    "print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68143710-5b0a-4b4d-bed4-4e3c9e0d3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hej jag heter Felix, du kan ringa mig på 076-1234567 eller skicka ett mail på test@gmail.com\n"
     ]
    }
   ],
   "source": [
    "nlp_config = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [\n",
    "        {\"lang_code\": \"sv\", \"model_name\": \"sv_core_news_md\"},\n",
    "         ],\n",
    "}\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=[\"PERSON\"],\n",
    "    languages_config=nlp_config,\n",
    ")\n",
    "\n",
    "print(\n",
    "    anonymizer.anonymize(\"Hej jag heter Felix, du kan ringa mig på 076-1234567 eller skicka ett mail på test@gmail.com\", language=\"sv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a1fefe4-66fb-4f33-8c7d-4037a11604fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "if OPENAI_API_KEY is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY does not exist, add it to env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c0da996-b896-4171-8d93-fa9dd33a1e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the OpenAI client\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Define your prompt template\n",
    "template = \"\"\"The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n",
    "\n",
    "User: {user_prompt}\n",
    "\n",
    "AI Assistant: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Initialize the LLMChain with the prompt and the OpenAI LLM client\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Enter the data input\n",
    "user_prompt = \"Hello\"\n",
    "\n",
    "# Run the user prompt through the chain\n",
    "response = llm_chain.run(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
